{"cells": [{"cell_type": "markdown", "id": "0700122e", "metadata": {}, "source": "# Routes.dat"}, {"cell_type": "markdown", "id": "dd152f3a", "metadata": {}, "source": "\n\n    Airline --> 2-letter (IATA) or 3-letter (ICAO) code of the airline.\n    Airline ID --> Unique OpenFlights identifier for airline (see Airline).\n    Source airport --> 3-letter (IATA) or 4-letter (ICAO) code of the source airport.\n    Source airport ID --> Unique OpenFlights identifier for source airport (see Airport)\n    Destination airport --> 3-letter (IATA) or 4-letter (ICAO) code of the destination airport.\n    Destination airport ID --> Unique OpenFlights identifier for destination airport (see Airport)\n    Codeshare --> \"Y\" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.\n    Stops --> Number of stops on this flight (\"0\" for direct)\n    Equipment --> 3-letter codes for plane type(s) generally used on this flight, separated by spaces\n\n    The data is UTF-8 encoded. The special value \\N is used for \"NULL\" to indicate that no value is available, and is understood automatically by MySQL if imported.\n\n\n"}, {"cell_type": "code", "execution_count": 3, "id": "6c893692", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Overwriting spark_analysis.py\n"}], "source": "%%writefile spark_analysis.py\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--bucket\", help=\"bucket for input and output\")\nargs = parser.parse_args()\n\nBUCKET = args.bucket"}, {"cell_type": "code", "execution_count": 4, "id": "c2d70c4f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nfrom pyspark.sql import SparkSession, SQLContext, Row\nspark = SparkSession.builder.appName(\"DataExploring\").getOrCreate()\nsc = spark.sparkContext\n\ndata_file = \"gs://{}/routes.dat\".format(BUCKET)\n\nroutes = sc.textFile(data_file).cache()\n\nroutes.take(5)"}, {"cell_type": "code", "execution_count": 7, "id": "f2fa2a8c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nroutes_split = routes.map(lambda row : row.split(\",\"))\nparsed_routes = routes_split.map(lambda r : Row(\n    airline = str(r[0]),\n    airline_ID = r[1],\n    source_airport = r[2],\n    source_airport_ID = r[3],\n    destination_airport = r[4],\n    destination_airport_ID = r[5],\n    codeshare = r[6],\n    stops = r[7],\n    equipment = r[8]\n    )\n)\n\nparsed_routes.take(5)"}, {"cell_type": "code", "execution_count": null, "id": "b552b0b2", "metadata": {}, "outputs": [], "source": "# airline_ID, destination_airport_ID, source_airport_ID"}, {"cell_type": "code", "execution_count": 8, "id": "1b361513", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n# parse edilmis dat dosyas\u0131n\u0131 dataframe'e cevirme \nsqlContext = SQLContext(sc)\nroutes_df = sqlContext.createDataFrame(parsed_routes)\nroutes_df.show(5)"}, {"cell_type": "code", "execution_count": 9, "id": "475832c7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nroutes_df.createTempView(\"routes\")"}, {"cell_type": "code", "execution_count": 10, "id": "31d08789", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nquery = spark.sql(\"\"\"\n              SELECT \n                  airline,\n                  destination_airport,\n                  COUNT(destination_airport) as Total_Destination_Flight\n              FROM routes\n              GROUP BY destination_airport, airline\n              ORDER BY Total_Destination_Flight DESC\n              LIMIT 20\n              \"\"\")\nquery.show()"}, {"cell_type": "code", "execution_count": 11, "id": "909fccf4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nax = query.sort(\"Total_Destination_Flight\").toPandas().plot.barh(x='airline', figsize=(10,10))"}, {"cell_type": "code", "execution_count": 12, "id": "65e4b448", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nax.get_figure().savefig('report_routes.png');"}, {"cell_type": "markdown", "id": "0c705432", "metadata": {}, "source": "## White House Visitor Records"}, {"cell_type": "markdown", "id": "b6d465f7", "metadata": {}, "source": "\n    UIN - Appointment Number \n    BDG NBR \u2013 Badge Number\n    Access Type - Type of access to the complex (VA = Visitor Access)\n    TOA \u2013 Time of Arrival\n    POA \u2013  Post of Arrival\n    TOD \u2013 Time of Departure \n    POD \u2013 Post of Departure\n    APPT_MADE_DATE \u2013 Date the Appointment was made.\n    APPT_START_DATE \u2013 Date and time for which the appointment was scheduled\n    APPT_END_DATE \u2013 Date and time for which the appointment was scheduled to end\n    APPT_CANCEL_DATE \u2013 Date the appointment was canceled, if applicable\n    Total_People- The total number of people scheduled for a particular appointment per requestor\n    LAST_UPDATEDBY \u2013 Identifier of officer that updated record\n    POST \u2013 Computer used to enter appointment\n    LastEntryDate \u2013 Most recent update to appointment\n    TERMINAL_SUFFIX - Identifier of officer that entered appointment\n    visitee_namelast \u2013 Last name of the visitee\n    visitee_namefirst \u2013 First name of the visitee\n    MEETING_LOC \u2013 Building in which meeting was scheduled\n    MEETING_ROOM \u2013 Room in which meeting was scheduled\n    CALLER_NAME_LAST \u2013 Last name of the individual that submitted the WAVES request\n    CALLER_NAME_FIRST \u2013 First name of the individual that submitted the WAVES request\n    CALLER_ROOM \u2013 Room from which the appointment was made \n    \n    Amac beyaz saraya gelen ziyaretciler en \u00e7ok kime gelmi\u015fler onu bulmak.\n    POTUS ge\u00e7en bir kolon var president of United States k\u0131saltmas\u0131 o kac\u0131nc\u0131 kolon ise o kolon kime gelindi\u011fini g\u00f6steriyor.\n"}, {"cell_type": "code", "execution_count": 13, "id": "593b815f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType, DateType\n\n\npath = \"gs://{}/whitehouse-waves-2014_03.csv\".format(BUCKET)\n    \nwhite_house_visitor_record = spark.read.option(\"header\", True).csv(path)\nwhite_house_visitor_record = white_house_visitor_record.withColumn(\"Total_People\", col(\"Total_People\").cast(IntegerType())) \\\n                             .withColumn(\"APPT_MADE_DATE\", col(\"APPT_MADE_DATE\").cast(DateType())) \\\n                             .withColumn(\"APPT_START_DATE\", col(\"APPT_START_DATE\").cast(DateType())) \\\n                             .withColumn(\"APPT_END_DATE\", col(\"APPT_END_DATE\").cast(DateType())) \\\n                             .withColumn(\"APPT_CANCEL_DATE\", col(\"APPT_CANCEL_DATE\").cast(DateType())) \\\n                             .withColumn(\"LASTENTRYDATE\", col(\"LASTENTRYDATE\").cast(DateType())) \\\n                             .withColumn(\"RELEASE_DATE\", col(\"RELEASE_DATE\").cast(DateType()))\n\nwhite_house_visitor_record.printSchema()"}, {"cell_type": "code", "execution_count": 14, "id": "e1034961", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nwhite_house_visitor_record.show(2)"}, {"cell_type": "code", "execution_count": null, "id": "9119e615", "metadata": {}, "outputs": [], "source": "# visitee_namefirst --> POTUS"}, {"cell_type": "code", "execution_count": 15, "id": "2051367f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nwhite_house_visitor_record.createTempView(\"white_house_visitor\")\nwhite_house_query = spark.sql(\"\"\"\n                        SELECT \n                            visitee_namefirst ||' '|| visitee_namelast as visite_fullname,\n                            COUNT(visitee_namefirst) as Total_Visit\n                        FROM white_house_visitor\n                        WHERE visitee_namelast != \"null\" and visitee_namefirst != 'VISITORS'\n                        GROUP BY visite_fullname\n                        ORDER BY Total_Visit DESC\n                        LIMIT 20\n                        \"\"\")\nwhite_house_query.show()"}, {"cell_type": "code", "execution_count": 16, "id": "73722ea3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nax = white_house_query.sort(\"Total_Visit\").toPandas().plot.barh(x='visite_fullname', figsize=(10,10))"}, {"cell_type": "code", "execution_count": 17, "id": "b70056c2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\nax.get_figure().savefig('report_white_house_visitors.png');"}, {"cell_type": "code", "execution_count": 18, "id": "a29e0d6f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nimport google.cloud.storage as gcs\nbucket = gcs.Client().get_bucket(BUCKET)\nfor blob in bucket.list_blobs(prefix='sparksqlondataproc/'):\n    blob.delete()\n\nfor fname in ['report_routes.png', 'report_white_house_visitors.png']:\n    bucket.blob('sparksqlondataproc/{}'.format(fname)).upload_from_filename(fname)\n\nbucket.blob('sparksqlondataproc/report_routes.png').upload_from_filename('report_routes.png')\nbucket.blob('sparksqlondataproc/report_white_house_visitors.png').upload_from_filename('report_white_house_visitors.png')"}, {"cell_type": "code", "execution_count": null, "id": "bf98ed5f", "metadata": {}, "outputs": [], "source": "#%%writefile -a spark_analysis.py\n#\n#connections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n#    \"gs://{}/sparksqlondataproc/connections_by_protocol\".format(BUCKET))\n#\n#connections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n#    \"gs://{}/sparksqlondataproc/connections_by_protocol\".format(BUCKET))"}, {"cell_type": "code", "execution_count": 20, "id": "7278a139", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing to de-projects-339514\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/01/29 13:36:43 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #0,5,main]) interrupted: \njava.lang.InterruptedException\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n22/01/29 13:36:43 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n|airline|airline_ID|source_airport|source_airport_ID|destination_airport|destination_airport_ID|codeshare|stops|equipment|\n+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n|     2B|       410|           AER|             2965|                KZN|                  2990|         |    0|      CR2|\n|     2B|       410|           ASF|             2966|                KZN|                  2990|         |    0|      CR2|\n|     2B|       410|           ASF|             2966|                MRV|                  2962|         |    0|      CR2|\n|     2B|       410|           CEK|             2968|                KZN|                  2990|         |    0|      CR2|\n|     2B|       410|           CEK|             2968|                OVB|                  4078|         |    0|      CR2|\n+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\nonly showing top 5 rows\n\n+-------+-------------------+------------------------+                          \n|airline|destination_airport|Total_Destination_Flight|\n+-------+-------------------+------------------------+\n|     TK|                IST|                     219|\n|     DL|                ATL|                     209|\n|     AA|                DFW|                     181|\n|     US|                DFW|                     177|\n|     LH|                FRA|                     169|\n|     AF|                CDG|                     164|\n|     UA|                IAH|                     161|\n|     AF|                ATL|                     161|\n|     KL|                ATL|                     160|\n|     UA|                ORD|                     158|\n|     CA|                PEK|                     143|\n|     UA|                DEN|                     141|\n|     UA|                EWR|                     140|\n|     KL|                AMS|                     137|\n|     AA|                CLT|                     133|\n|     US|                CLT|                     133|\n|     SU|                SVO|                     131|\n|     DL|                DTW|                     131|\n|     BA|                LHR|                     130|\n|     LH|                MUC|                     129|\n+-------+-------------------+------------------------+\n\nroot                                                                            \n |-- NAMELAST: string (nullable = true)\n |-- NAMEFIRST: string (nullable = true)\n |-- NAMEMID: string (nullable = true)\n |-- UIN: string (nullable = true)\n |-- BDGNBR: string (nullable = true)\n |-- ACCESS_TYPE: string (nullable = true)\n |-- TOA: string (nullable = true)\n |-- POA: string (nullable = true)\n |-- TOD: string (nullable = true)\n |-- POD: string (nullable = true)\n |-- APPT_MADE_DATE: date (nullable = true)\n |-- APPT_START_DATE: date (nullable = true)\n |-- APPT_END_DATE: date (nullable = true)\n |-- APPT_CANCEL_DATE: date (nullable = true)\n |-- Total_People: integer (nullable = true)\n |-- LAST_UPDATEDBY: string (nullable = true)\n |-- POST: string (nullable = true)\n |-- LASTENTRYDATE: date (nullable = true)\n |-- TERMINAL_SUFFIX: string (nullable = true)\n |-- visitee_namelast: string (nullable = true)\n |-- visitee_namefirst: string (nullable = true)\n |-- MEETING_LOC: string (nullable = true)\n |-- MEETING_ROOM: string (nullable = true)\n |-- CALLER_NAME_LAST: string (nullable = true)\n |-- CALLER_NAME_FIRST: string (nullable = true)\n |-- CALLER_ROOM: string (nullable = true)\n |-- description: string (nullable = true)\n |-- RELEASE_DATE: date (nullable = true)\n\n22/01/29 13:37:12 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n+--------+---------+-------+------+------+-----------+----+----+----+----+--------------+---------------+-------------+----------------+------------+--------------+----+-------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+-----------+------------+\n|NAMELAST|NAMEFIRST|NAMEMID|   UIN|BDGNBR|ACCESS_TYPE| TOA| POA| TOD| POD|APPT_MADE_DATE|APPT_START_DATE|APPT_END_DATE|APPT_CANCEL_DATE|Total_People|LAST_UPDATEDBY|POST|LASTENTRYDATE|TERMINAL_SUFFIX|visitee_namelast|visitee_namefirst|MEETING_LOC|MEETING_ROOM|CALLER_NAME_LAST|CALLER_NAME_FIRST|CALLER_ROOM|description|RELEASE_DATE|\n+--------+---------+-------+------+------+-----------+----+----+----+----+--------------+---------------+-------------+----------------+------------+--------------+----+-------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+-----------+------------+\n|  Aalami|  Arianna|      N|U44019|  null|         VA|null|null|null|null|          null|           null|         null|            null|         275|            JC| WIN|         null|             JC|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|       null|        null|\n|  Aalami|  Florian|      B|U44019|  null|         VA|null|null|null|null|          null|           null|         null|            null|         275|            JC| WIN|         null|             JC|          OFFICE|         VISITORS|         WH|   RESIDENCE|          OFFICE|         VISITORS|       null|       null|        null|\n+--------+---------+-------+------+------+-----------+----+----+----+----+--------------+---------------+-------------+----------------+------------+--------------+----+-------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+-----------+------------+\nonly showing top 2 rows\n\n+----------------+-----------+                                                  \n| visite_fullname|Total_Visit|\n+----------------+-----------+\n|   Gene Sperling|        979|\n|       Dan Utech|        974|\n| Gautam Raghavan|        861|\n|       Sam Brown|        779|\n|  Melissa Rogers|        758|\n|    Rumana Ahmed|        735|\n|    John Podesta|        676|\n|     J.J. Raynor|        656|\n|      Bess Evans|        621|\n|   Amy Rosenbaum|        572|\n|LADINI JAYARATNE|        565|\n|    Allison Hunn|        559|\n|  Lynn Rosenthal|        491|\n|      Wayne Ting|        471|\n|  Heather Foster|        453|\n|  Tonya Robinson|        449|\n|        Gabe Amo|        421|\n|     David Agnew|        414|\n|    Ashwani Jain|        408|\n|    Nate Perkins|        397|\n+----------------+-----------+\n\n"}, {"name": "stdout", "output_type": "stream", "text": "                                                                                \r"}], "source": "BUCKET= 'de-projects-339514'  # CHANGE\nprint('Writing to {}'.format(BUCKET))\n!python spark_analysis.py --bucket=$BUCKET"}, {"cell_type": "code", "execution_count": 22, "id": "9135c799", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://de-projects-339514/Project2.ipynb\r\ngs://de-projects-339514/routes.dat\r\ngs://de-projects-339514/sparksqlondataproc/report_routes.png\r\ngs://de-projects-339514/sparksqlondataproc/report_white_house_visitors.png\r\ngs://de-projects-339514/whitehouse-waves-2014_03.csv\r\n"}], "source": "!gsutil ls gs://$BUCKET/**"}, {"cell_type": "code", "execution_count": 6, "id": "1235b3cf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "de-projects-339514\r\n"}], "source": "!gcloud info --format='value(config.project)'"}, {"cell_type": "code", "execution_count": 23, "id": "c8a0fe66", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying file://spark_analysis.py [Content-Type=text/x-python]...\n/ [1 files][  3.7 KiB/  3.7 KiB]                                                \nOperation completed over 1 objects/3.7 KiB.                                      \n"}], "source": "!gsutil cp spark_analysis.py gs://de-projects-339514/sparksqlondataproc/spark_analysis.py"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}